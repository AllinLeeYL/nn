# 卷积神经网络

## **一、概述**

基础的**CNN**由 **卷积**(convolution), **激活**(activation), and **池化**(pooling)三种结构组成。CNN输出的结果是每幅图像的特定特征空间。当处理图像分类任务时，我们会把CNN输出的特征空间作为**全连接层或全连接神经网络**(fully connected neural network, FCN)的输入，用全连接层来完成从输入图像到标签集的映射，即分类。整个过程最重要的工作就是如何通过训练数据迭代调整网络权重，也就是**反向传播算法**

此次实验我们小组根据MNIST数据集的特征，设计了如下顺序的卷积神经网络：

**输入层**：N\*28\*28的原始数据，其中N代表训练集的图片数量。

**卷积层**：输入层结果为输入，3\*5\*5的卷积核，3\*24\*24的输出，激活函数采用sigmoid

**池化层**：卷积层结果为输入，2\*2的池化矩阵，3\*12\*12的池化层输出。

**全连接层**（输出层）：池化层结果为输入，激活函数采用softmax，输出为3类（0,1,2）对应的概率值。

**BP反向传播**：损失函数为交叉熵代价函数，从输出层依次向前求导修改权值和偏置。

补充：在卷积层和全连接层计算的时候采用了将矩阵拉伸成向量的方式加速计算。

整个网络结构大致如下图（以一张图为例子）：

![1](pic/网络框架图.png)

## 二、输入层

### 1	原理分析

本次实验采用的是TensorFlow datasets中的mnist手写字：mnist返回为两个数组，第一个是一个n\*m维的 NumPy array，这里的 n 是样本数(行数)， m 是特征数(列数)。训练数据集包含 60,000 个样本， 测试数据集包含 10,000 样本。

 在 MNIST 数据集中的每张图片由 28 x 28 个像素点构成， 每个像素点用一个灰度值表示。我们将 28 x 28 的像素展开为一个一维的行向量，这些行向量就是图片数组里的行（每行 784 个值, 或者说每行就是代表了一张图片）。返回的第二个数组（labels） 包含了相应的目标变量，也就是手写数字的类标签(整数 0-9)。

本次实验要求仅使用到0,1,2的数据，因此在取训练集和测试集的时候会先对数据进行预处理。预处理部分还需要将图片数据从一维的1\*784转化成二维的28\*28，以便于后面计算使用。

### 2	关键代码实现

调用TensorFlow库获取数据集，并且按照shuffle的方式随机抽取加载，获得原始数据。

```python
data_mix = data_mix.shuffle(1024).batch(128).repeat(5).prefetch(10)
```

再将其抽取0,1,2数据进行保存，并且在数量达到预设值时候返回

```python
for i in range(labels.shape[0]):
    if labels[i] == 0 or labels[i] == 1 or labels[i] == 2:
        ret_data[total_count] = data[i]
        ret_label[total_count] = labels[i]
        total_count += 1
        #当数量达到要求的就返回
        if total_count == N:
            return ret_data,ret_label
```

## 三、卷积层

卷积层是整个卷积神经网络的核心部分，也是他与其他神经网络的区别所在。

### 1	原理分析

#### 1.1	卷积定义

CNN中最基础的操作是卷积convolution，再精确一点，基础CNN所用的卷积是一种2-D卷积。也就是说，kernel只能在x,y上滑动位移，不能进行深度 (跨通道) 位移。卷积层由一组滤波器组成，滤波器可以视为二维数字矩阵。

我们可以将滤波器与输入图像进行卷积来产生输出图像，具体的步骤如下：

1. 在图像的某个位置上覆盖滤波器；
2. 将滤波器中的值与图像中的对应像素的值相乘；
3. 把上面的乘积加起来，得到的和是输出图像中目标像素的值；
4. 对图像的所有位置重复此操作。

通常，卷积有助于我们找到特定的局部图像特征（如边缘），用在后面的网络中。

#### 1.2	卷积优点

卷积的优势主要体现在两点上：**1.局部感知，2.权值共享**

##### 1.2.1	局部感知

一般认为人对外界的认知是从局部到全局的，而图像的空间联系也是局部的像素联系较为紧密，而距离较远的像素相关性则较弱。因而，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。如下图则将3\*3的局部信息汇总到了1个神经元上，此时不仅能提取出局部的相关联信息，更重要的是减少了大量距离较远的神经元的，且不必要的计算。同时利用多个卷积核，则可以获取多个不同的特征用于提高最终分类的准确率。

##### 1.2.2	权值共享

这里的权值更多指的是卷积核上的权值。图像的一部分的统计特性与其他部分是一样的。这也意味着我们在这一部分学习的特征也能用在另一部分上，所以对于这个图像上的所有位置，我们都能使用同样的学习特征。这能大量减少计算的权值量，大幅提升模型训练的效率。

<img src="pic/卷积原理图.png" alt="2" style="zoom:30%;" />

#### 1.3	卷积的参数

一般的卷积操作有如下几个常用的参数（公式）：

1. 输入体积的大小为
   $$
   H_1*W_1*D_1
   $$
   即原图像的长度、宽度和深度。

2. 四个超参数：

   a）.卷积核的数量K

   b）.卷积核的大小F

   c）.步长S

   d）.零填充大小P，其中零填充用于将卷积后的图像用0填充使得图像与原来大小相同。

3. 输出体积大小：
   $$
   H_2*W_2*D_2
   $$
   其中满足：
   $$
   H_2 = \frac{H_1 - F + 2P}{S} + 1\\
   W_2 = \frac{W_1 - F + 2P}{S} + 1\\
   D_2=K
   $$
   
4. 

### 2	关键代码实现

为了实现卷积的计算，我们代码的实现主要分成两部分：（此处相当于将卷积核分为形状和权值两部分，形状的计算通过a实现，权值通过b实现）

​	a）.将卷积核拉成向量，并且与图片数据进行卷积，这一部分实现通过im2col函数实现。

​	b）.将im2col计算后的结果再与权值和偏置进行线性变换，再通过sigmoid函数进行激活，得到输出。

#### 2.1	im2col实现

im2col实际是用于优化卷积运算，对于卷积核每一次要处理的小窗，将其展开到新矩阵的一行（列），新矩阵的列（行）数，就是对于一副输入图像，卷积运算的次数（卷积核滑动的次数），下面为示例图：

<img src="pic/im2col原理图.png" alt="3" style="zoom:30%;" />

im2col实现代码：

1.首先通过原理分析中的公式，计算出卷积后矩阵的大小：

```python
out_h = int((H - core_h + pad * 2) / stride + 1)  # 纵向上能匹配卷积核几次
out_w = int((W - core_w + pad * 2) / stride + 1)  # 横向上能匹配卷积核几次
```

2.同时根据im2col的原理我们可以得到输出结果应该为“卷积后矩阵的大小\*卷积核的大小”：

```python
result = np.zeros((out_h * out_w, core_w * core_h))  # 每张图片拉伸之后的结果应该是如下矩阵：(匹配的卷积核次数，一个卷积核中的元素个数)
```

3.针对于每一次卷积核的匹配，保存拉伸后的向量存储到result对应的列中，并且最终返回im2col的结果。

```python
for w in range(out_w):
    max_w = w * stride + core_w
    for h in range(out_h):  # 在矩阵上移动卷积核
        max_h = h * stride + core_h
        i = 0
        for x in range(w * stride, max_w):
            for y in range(h * stride, max_h):  # 将卷积核内的小矩阵拉伸成一维向量
                result[w * h, i] = pad_image[x, y]
                i += 1
return result
```

#### 2.2	convolution实现

convolution实现相对简单，与一般的全连接神经网络计算方法类似，主要就是将im2col的结果与权值W进行向量乘积，再加上偏置矩阵即可。

```python
result = np.dot(X, W)
result += b  # WX+b
```

还需要通过sigmoid函数对结果进行激活：
$$
S(x)=\frac{1}{1+e^{-x}}
$$

```python
def sigmoid(X):
    for i in range(X.shape[0]):
        X[i][0] = 1 / (1 + exp(-X[i][0]))
    return X
```

## 四、池化层

### 1	原理分析

#### 1.1	池化定义

在卷积神经网络中通常会在相邻的卷积层之间加入一个池化层，池化层可以有效的缩小参数矩阵的尺寸，从而减少最后连接层的中的参数数量。所以加入池化层可以加快计算速度和防止过拟合的作用。

池化的原理或者是过程：pooling是在不同的通道上分开执行的（就是池化操作不改变通道数），且不需要参数控制。然后根据窗口大小进行相应的操作。 一般有max pooling、average pooling等。

max pooling取每一个窗口中的最大值，作为该窗口的特征值传递给下一层。

average pooling取每一个窗口中所有的值的平均值，作为该窗口的特征值传递给下一层。

<img src="pic/池化原理图.png" alt="4" style="zoom:30%;" />

#### 1.2	池化意义

1. 通过池化层可以减少空间信息的大小,也就提高了运算效率;
2. 减少空间信息也就意味着减少参数,这也降低了overfit的风险;
3. 获得空间变换不变性(translation rotation scale invarance,平移旋转缩放的不变性);

### 2	关键代码实现

1.由于在卷积层我们通过了im2col的方式进行计算，因此当前卷积层输出的结果是一个一维向量（对于单张图片）。为了便于我们实现pooling的功能，我们需要先把向量拉伸回一个24\*24的矩阵，再进行pooling操作，相当于实现了一个col2im的功能。

```python
for i in range(X.shape[0]):
    temp[int(i / W), i % W] = X[i][0]  # 首先把向量还原回矩阵结构
```

2.然后我们可以通过计算得到池化后矩阵的大小并初始化：PH为池化矩阵的高，PW为池化矩阵的宽。
$$
H_2 = \frac{H_1}{PH}\\
W_2 = \frac{W_1}{PW}\\
$$

```python
nw = int(W / pool_w)
nh = int(H / pool_h)  # 按池化将卷积后的矩阵按池化矩阵分割
result = np.zeros((nw, nh))  # 池化后的矩阵大小
```

3.最后再通过循环的方式获取对应池化窗口中的值（此处采用average pooling的方式）：

```python
for w in range(0, W, pool_w):
    for h in range(0, H, pool_h):  # 在大矩阵内移动池化矩阵
        max_w = w + pool_w
        max_h = h + pool_h
        aveg = 0
        for x in range(w, max_w):
            for y in range(h, max_h):  # 每个池化矩阵求其内所有元素的平均值
                aveg += temp[x, y]
        aveg /= (pool_w * pool_h)
        result[int(write / nw), write % nw] = aveg
        write += 1
return result
```

## 五、全连接层（输出层）

出现在CNN中的全连接网络(fully connected network)主要目的是为了分类, 这里称它为network的原因是，目前CNNs多数会采用多层全连接层，这样的结构可以被认为是网络。而本次实验我们采用的全连接层是一层，且直接作为输出层使用，所以全连接层的结果就是0,1,2三类对应的预测值。

### 1	原理分析

#### 1.1全连接定义

全连接层（fully connected layers，FC）在整个卷积神经网络中起到“分类器”的作用。在 CNN 中，全连接常出现在最后几层，用于对前面设计的特征做加权和。前面的卷积和池化相当于做特征工程，后面的全连接相当于做特征加权。

全连接的核心操作就是矩阵向量乘积，
$$
y=\omega x +b
$$
本质就是由一个特征空间线性变换到另一个特征空间。目标空间的任一维——也就是隐层的一个 cell——都认为会受到源空间的每一维的影响。不考虑严谨，可以说，目标向量是源向量的加权和。

在实际使用中，全连接层可由卷积操作实现：对前层是全连接的全连接层可以转化为卷积核为1x1的卷积；而前层是卷积层的全连接层可以转化为卷积核为hxw的全局卷积，h和w分别为前层卷积结果的高和宽。

此处以猫的特征为例子显示全连接的特点：

<img src="pic/全连接原理图.png" alt="5" style="zoom:60%;" />

#### 1.2	softmax激活函数

由于该层要作为输出层，因此需要使用到激活函数以使得最终能得到三类分类的对应预测值，我们本次采用的是softmax函数作为输出层的激活函数。

softmax函数，又称**归一化指数函数。**它是二分类函数sigmoid在多分类上的推广，目的是将多分类的结果以概率的形式展现出来，其定义如下：
$$
S_i = \frac{e^i}{\sum_je^j}
$$
我们可以将它理解成通过函数激活的方式求出每个值对应的概率，在最后选取输出结点的时候，我们就可以选取概率最大（也就是值对应最大的）结点，作为我们的预测目标。

### 2	关键代码实现

实际上我们可以将全连接理解为利用与上一层输出的矩阵大小（在这里指池化后的12\*12矩阵）相同的卷积核进行全局卷积操作。在实际的代码实现我们确实就是采用这种方法，因此我们依然需要先对数据进行im2col的操作，传入与3个12\*12的卷积核进行向量的拉伸，然后再通过与权值和偏置，加上softmax函数得到最终的结果。

由于这部分实现与卷积层实现相似，便不赘述代码（此处仅展示softmax函数实现）

```python
def softmax(X):
    sum = 0.
    for i in range(X.shape[0]):
        sum += np.exp(X[i])
    result = []
    for i in range(X.shape[0]):
        result.append(np.exp(X[i]) / sum)
    return result
```

